\chapter{Related Work}

\section{Why Johnny Can't Pentest Related Work}
\label{sec:related_work}

Our work is related to two main areas of research: the design of 
web applications for assessing vulnerability analysis tools and the
evaluation of web scanners.

\noindent {\bf Designing test web applications.} 
Vulnerable test applications are required to assess 
web vulnerability scanners. Unfortunately, no standard
test suite is currently available or accepted by the industry. 
HacmeBank~\cite{hacme_bank} and WebGoat~\cite{owasp-webgoat} are two
well-known, publicly-available, vulnerable web applications, but their
design is focused more on teaching web application security rather
than testing automated scanners. 
SiteGenerator~\cite{owasp_sitegenerator} is a tool to
generate sites with certain characteristics (e.g., classes of
vulnerabilities) according to its input configuration. 
While SiteGenerator is useful to automatically produce different vulnerable
sites, we found it easier to manually introduce in Wacko\-Picko the
vulnerabilities with the characteristics that we wanted to test.

\noindent {\bf Evaluating web vulnerability scanners.}
There exists a growing body of literature on the evaluation of web
vulnerability scanners. For example, 
Suto compared three scanners against three different applications and
used code coverage, among other metrics, as a measure of the
effectiveness of each scanner~\cite{suto07}. In a recent follow-up study,
Suto~\cite{suto10:webscanners} assessed seven scanners and compared
their detection capabilities and the time required to run them.
Wiegenstein et al. ran five unnamed scanners against a custom
benchmark~\cite{wiegenstein06}. Unfortunately, the authors do not discuss in
detail the reasons for detections or spidering failures.
In their survey of web security assessment tools, Curphey
and Araujo reported that black-box scanners perform
poorly~\cite{curphey06}.
Peine examined in depth the functionality and user interfaces of seven
scanners (three commercial) that were run against WebGoat
and one real-world application~\cite{peine06}.
Kals et al. developed a new web vulnerability scanner and tested it on
about 25,000 live web pages~\cite{kals06:secubat}. Since no ground
truth is available for these sites, the authors cannot discuss false
negative rate or failures of their tool.
More recently, AnantaSec released an evaluation of three scanners against
13 real-world applications, three web applications provided by the 
scanners vendors, and a series of JavaScript tests~\cite{anantasec09}. 
While this experiment assesses a large number of real-world
applications, only a limited number of scanners are tested and no
explanation is given for the results.
In addition, Vieira et al. tested four web scanners on 300 web
services~\cite{vieira09}. They also report high rates of false
positives and false negatives.

In comparison, our work, to the best of our knowledge, performs the largest evaluation of web
application scanners in terms of the number of tested tools (eleven,
both commercial and open-source), and the class of vulnerabilities analyzed.
In addition, we discuss the effectiveness of different
configurations and levels of manual intervention, and examine in detail
the reasons for a scanner's success or failure.

Furthermore, we provide a discussion of challenges (i.e., critical
limitations) of current web vulnerability scanners. While some of these
problem areas were discussed before ~\cite{grossman04,mcallister08}, we provide quantitative
evidence that these issues are actually limiting the performance of
today's tools.
We believe that this discussion will provide useful insight into
how to improve state-of-the-art of black-box web vulnerability scanners.

